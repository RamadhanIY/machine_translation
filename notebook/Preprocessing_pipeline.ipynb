{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6EuO6SYolr"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIhdyxlbvHz-"
      },
      "outputs": [],
      "source": [
        "pip install bpemb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZA1rTANthes"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from bpemb import BPEmb\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from string import punctuation\n",
        "import pickle\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set,Optional\n",
        "from pathlib import Path\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import pickle\n",
        "import re\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9bcyUQB8Mll",
        "outputId": "3c586122-e1e0-46ef-8cf4-d105c593d964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqM5V3k3P_yo"
      },
      "source": [
        "# Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZwwoc3dQBV2"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Dict\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "@dataclass\n",
        "class PreprocessingConfig:\n",
        "    # Text Cleaning Parameters\n",
        "    lowercase: bool = True\n",
        "    remove_punctuation: bool = True\n",
        "    remove_urls: bool = True\n",
        "    remove_html_tags: bool = True\n",
        "    remove_emails: bool = True\n",
        "    remove_quotes: bool = True\n",
        "    min_word_length: int = 2\n",
        "    max_sentence_length: int = 35\n",
        "\n",
        "    # Slang Replacement\n",
        "    enable_slang_replacement: bool = True\n",
        "    slang_dictionaries: List[str] = field(default_factory=lambda: [\n",
        "        '/content/drive/MyDrive/Dataset/MT-JavaIndo/new_kamusalay.csv',\n",
        "        '/content/drive/MyDrive/Dataset/MT-JavaIndo/inforformal-formal-Indonesian-dictionary.tsv',\n",
        "        '/content/drive/MyDrive/Dataset/MT-JavaIndo/dataset cerpen/kamus_alay_versi2.csv'\n",
        "    ])\n",
        "\n",
        "    # Normalization\n",
        "    unicode_normalization: bool = True\n",
        "    remove_diacritics: bool = True\n",
        "\n",
        "    # Additional preprocessing specifics\n",
        "    punctuation_to_remove: str = r'[%.,?!\":;()[]\"-=@©Ã$¨*‰]'\n",
        "    special_char_regex: str = r\"[^\\w\\s-]\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokenization_method: str = \"word_tokenize\"\n",
        "    oov_token: str = \"<OOV>\"\n",
        "    padding_type: str = \"post\"\n",
        "\n",
        "    # Vocabulary\n",
        "    max_vocab_size: Optional[int] = None\n",
        "    min_word_frequency: int = 1\n",
        "\n",
        "    # Data Splitting\n",
        "    test_size: float = 0.1\n",
        "    random_state: int = 42\n",
        "    k_fold_splits: int = 5\n",
        "    k_fold_shuffle: bool = True\n",
        "\n",
        "    # Dataset Flags\n",
        "    data_nusa_writes: bool = False\n",
        "    data_nusax: bool = False\n",
        "    data_korpus_nusantara: bool = False\n",
        "    data_final: bool = False\n",
        "\n",
        "\n",
        "class ConfigurablePreprocessor:\n",
        "    def __init__(self, config: PreprocessingConfig = None):\n",
        "        self.config = config or PreprocessingConfig()\n",
        "        self.slang_dictionaries = {}\n",
        "\n",
        "    def load_slang_dictionary(self, label: str = 'indo') -> Dict[str, str]:\n",
        "      try:\n",
        "          # Load Indonesian slang dictionaries\n",
        "          if label == 'indo':\n",
        "              slang_dicts = []\n",
        "              for dict_path in self.config.slang_dictionaries:\n",
        "                  try:\n",
        "                      if dict_path.endswith('.csv'):  # Assuming CSV for most cases\n",
        "                          df = pd.read_csv(dict_path, encoding='latin1', names=[\"slang\", \"formal\"])\n",
        "                          slang_dicts.append(df)\n",
        "                      elif dict_path.endswith('.tsv'):  # For tab-separated values\n",
        "                          df = pd.read_csv(dict_path, sep='\\t', header=0)\n",
        "                          df = df.rename(columns={'informal': 'slang'})  # Adjust column name\n",
        "                          slang_dicts.append(df)\n",
        "                  except FileNotFoundError:\n",
        "                      print(f\"Warning: Dictionary {dict_path} not found.\")\n",
        "\n",
        "              # Concatenate all dictionaries\n",
        "              if slang_dicts:\n",
        "                  slang_dict = pd.concat(slang_dicts, ignore_index=True)\n",
        "                  return dict(zip(slang_dict['slang'], slang_dict['formal']))\n",
        "              else:\n",
        "                  print(\"No slang dictionaries were loaded.\")\n",
        "                  return {}\n",
        "\n",
        "          # Load Javanese slang dictionary\n",
        "          elif label == 'java':\n",
        "              dualisme_dict = pd.read_csv(\"/content/drive/MyDrive/Dataset/MT-JavaIndo/nusa/dict_dualisme.csv\")\n",
        "              return dict(zip(dualisme_dict['dualisme'], dualisme_dict['usedword']))\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error loading slang dictionary: {e}\")\n",
        "          return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text: str, label: str) -> str:\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        text = str(text)\n",
        "\n",
        "        if self.config.enable_slang_replacement:\n",
        "            slang_dict = self.load_slang_dictionary(label)\n",
        "        else:\n",
        "            slang_dict = {}\n",
        "\n",
        "        if self.config.lowercase:\n",
        "            text = text.lower()\n",
        "        if self.config.remove_punctuation:\n",
        "            text = re.sub(self.config.punctuation_to_remove, '', text)\n",
        "        if self.config.remove_urls:\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        if self.config.remove_html_tags:\n",
        "            text = re.sub(r'<.*?>', '', text)\n",
        "        if self.config.remove_emails:\n",
        "            text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        if self.config.remove_quotes:\n",
        "            text = text.strip('\"').strip('\"').strip('\"')\n",
        "\n",
        "        words = text.split()\n",
        "        words = [slang_dict.get(word, \"\") if slang_dict.get(word) == \"<nan>\" else slang_dict.get(word,word) for word in words if word]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "        text = re.sub(self.config.special_char_regex, \"\", text)\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        if self.config.min_word_length > 1:\n",
        "            text = ' '.join([w for w in text.split() if len(w) >= self.config.min_word_length])\n",
        "\n",
        "        if self.config.unicode_normalization:\n",
        "            normalized_text = unicodedata.normalize('NFKD', text)\n",
        "            text = ''.join([c for c in normalized_text if not unicodedata.combining(c)])\n",
        "\n",
        "        return text\n",
        "\n",
        "    def call_data(self, config: PreprocessingConfig) -> pd.DataFrame:\n",
        "        config = config or self.config  # Use self.config if no config is passed\n",
        "        datasets = []\n",
        "        if config.data_nusa_writes:\n",
        "            try:\n",
        "                data_nusa_alinea =  pd.read_csv(\"/content/drive/MyDrive/Dataset/MT-JavaIndo/nusa/nusa-alinea-15k.csv\")\n",
        "                data_nusa_alinea = data_nusa_alinea.rename(columns={'indo': 'Indonesian','jawa':'Javanese'})\n",
        "                data_nusa_alinea = data_nusa_alinea[['Javanese', 'Indonesian'] + [col for col in data_nusa_alinea.columns if col not in ['Javanese', 'Indonesian']]]\n",
        "                data_nusa_alinea = data_nusa_alinea.rename(columns={'Indonesian': 'label', 'Javanese': 'text'})\n",
        "                datasets.append(data_nusa_alinea)\n",
        "            except FileNotFoundError:\n",
        "                print(\"Warning: nusa-alinea-15k.csv not found.\")\n",
        "        if config.data_nusax:\n",
        "            try:\n",
        "                datasets.append(pd.read_csv('/content/drive/MyDrive/Dataset/MT-JavaIndo/nusax_data.csv'))\n",
        "            except FileNotFoundError:\n",
        "                print(\"Warning: nusax_data.csv not found.\")\n",
        "        if config.data_korpus_nusantara:\n",
        "            try:\n",
        "                datasets.append(pd.read_csv('/content/drive/MyDrive/Dataset/MT-JavaIndo/korpus_nusantara_preprocessed.csv'))\n",
        "            except FileNotFoundError:\n",
        "                print(\"Warning: korpus_nusantara_preprocessed.csv not found.\")\n",
        "        if config.data_final:\n",
        "            try:\n",
        "                datasets.append(pd.read_csv(\"/content/drive/MyDrive/Dataset/MT-JavaIndo/final_data.csv\"))\n",
        "            except FileNotFoundError:\n",
        "                print(\"Warning: final_data.csv not found.\")\n",
        "\n",
        "        if datasets:\n",
        "            print(\"datasets has been combined, going to save as csv with name of combined_data_final.csv\")\n",
        "            dataset_final = pd.concat(datasets, ignore_index=True)\n",
        "            # dataset_final.drop( axis=1, inplace=True)\n",
        "            dataset_final.to_csv('/content/drive/MyDrive/Dataset/MT-JavaIndo/combined_data_final.csv')\n",
        "            # raise ValueError(\"datasets has been combined, going to save as csv with name of combined_data_final.csv\")\n",
        "            return dataset_final\n",
        "        else:\n",
        "            raise ValueError(\"No datasets were loaded. Please check the configuration.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm74Gp_zccdh",
        "outputId": "0e6cc9c6-9cbe-4bdc-e195-e4baed273e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined slang dictionary loaded:\n",
            "                 slang                    formal\n",
            "0  anakjakartaasikasik  anak jakarta asyik asyik\n",
            "1         pakcikdahtua         pak cik sudah tua\n",
            "2       pakcikmudalagi         pak cik muda lagi\n",
            "3          t3tapjokowi              tetap jokowi\n",
            "4                   3x                 tiga kali\n",
            "Full loaded slang dictionary:\n",
            "                     slang                    formal\n",
            "0      anakjakartaasikasik  anak jakarta asyik asyik\n",
            "1             pakcikdahtua         pak cik sudah tua\n",
            "2           pakcikmudalagi         pak cik muda lagi\n",
            "3              t3tapjokowi              tetap jokowi\n",
            "4                       3x                 tiga kali\n",
            "...                    ...                       ...\n",
            "19209        kedatangan-mu              kedatanganmu\n",
            "19210        sejahtera-nya              sejahteranya\n",
            "19211       dikunjungi-nya             dikunjunginya\n",
            "19212              yoÃÂ«l                     <nan>\n",
            "19213          kaucurahkan              kau curahkan\n",
            "\n",
            "[19214 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from typing import Dict\n",
        "\n",
        "class SlangLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_slang_dictionary(self, label: str = 'indo') -> pd.DataFrame:\n",
        "        try:\n",
        "            # Load Indonesian slang dictionaries\n",
        "            if label == 'indo':\n",
        "                slang_dicts = []\n",
        "                for dict_path in self.config['slang_dictionaries']:  # Mengakses dictionary langsung\n",
        "                    try:\n",
        "                        if dict_path.endswith('.csv'):  # Assuming CSV for most cases\n",
        "                            df = pd.read_csv(dict_path, encoding='latin1', names=[\"slang\", \"formal\"])\n",
        "                            slang_dicts.append(df)\n",
        "                        elif dict_path.endswith('.tsv'):  # For tab-separated values\n",
        "                            df = pd.read_csv(dict_path, sep='\\t', header=0)\n",
        "                            df = df.rename(columns={'informal': 'slang'})  # Adjust column name\n",
        "                            slang_dicts.append(df)\n",
        "                    except FileNotFoundError:\n",
        "                        print(f\"Warning: Dictionary {dict_path} not found.\")\n",
        "\n",
        "                # Concatenate all dictionaries into one DataFrame\n",
        "                if slang_dicts:\n",
        "                    combined_df = pd.concat(slang_dicts, ignore_index=True)\n",
        "                    # Print the first few rows to verify\n",
        "                    print(\"Combined slang dictionary loaded:\")\n",
        "                    print(combined_df.head())  # Show first few rows\n",
        "                    return combined_df\n",
        "                else:\n",
        "                    print(\"No slang dictionaries were loaded.\")\n",
        "                    return pd.DataFrame()  # Return an empty DataFrame if no dictionaries loaded\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading slang dictionary: {e}\")\n",
        "            return pd.DataFrame()  # Return empty DataFrame in case of error\n",
        "\n",
        "# Contoh pengujian dengan konfigurasi\n",
        "config = {\n",
        "    \"slang_dictionaries\": [\n",
        "        \"/content/drive/MyDrive/Dataset/MT-JavaIndo/new_kamusalay.csv\",\n",
        "        \"/content/drive/MyDrive/Dataset/MT-JavaIndo/inforformal-formal-Indonesian-dictionary.tsv\",\n",
        "        \"/content/drive/MyDrive/Dataset/MT-JavaIndo/dataset cerpen/kamus_alay_versi2.csv\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Menginisialisasi objek dan memanggil fungsi\n",
        "slang_loader = SlangLoader(config)\n",
        "df_slang = slang_loader.load_slang_dictionary('indo')\n",
        "\n",
        "# Jika ingin melihat seluruh DataFrame, bisa print seluruhnya\n",
        "print(\"Full loaded slang dictionary:\")\n",
        "print(df_slang)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elS9NGH5cqFE",
        "outputId": "9fef87b5-d584-4957-9a43-260406470cfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "slang     0\n",
              "formal    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>slang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formal</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "df_slang.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQnhNc_QMjD"
      },
      "source": [
        "# Preprocess Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjSrRE7ZQNU0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to add start and end tokens to labels\n",
        "def add_start_end_tokens(data, column):\n",
        "    data[column] = data[column].apply(lambda x: f'<start> {x} <end>')\n",
        "\n",
        "# Function to calculate vocabulary size\n",
        "def get_vocab(data, column):\n",
        "    vocab = defaultdict(list)\n",
        "    for index, sentence in enumerate(data[column]):\n",
        "        for word in sentence.split():\n",
        "            vocab[word].append(sentence)  # Append the sentence to the word's list\n",
        "    return vocab\n",
        "\n",
        "# Function to encode and pad sequences\n",
        "def encode_and_pad(tokenizer, data, length, padding_type='post',lang = None):\n",
        "    vocab_size=10000\n",
        "    bpemb = BPEmb(lang=lang, vs=vocab_size)\n",
        "\n",
        "    # Tokenize data using BPEmb\n",
        "    bpemb_data = [' '.join(bpemb.encode(text)) for text in data]\n",
        "    sequences = tokenizer.texts_to_sequences(bpemb_data)\n",
        "    return pad_sequences(sequences, maxlen = length, padding=padding_type)\n",
        "\n",
        "def build_tokenizer(data,tokenization_method, oov_token, max_vocab_size,lang = None):\n",
        "    # Load BPEmb Javanese model\n",
        "\n",
        "    vocab_size=10000\n",
        "    bpemb = BPEmb(lang=lang, vs=vocab_size)\n",
        "\n",
        "    # Tokenize data using BPEmb\n",
        "    tokenized_data = [' '.join(bpemb.encode(text)) for text in data]\n",
        "\n",
        "    # Create Keras Tokenizer\n",
        "    tokenizer = Tokenizer(oov_token=oov_token, filters='')\n",
        "    tokenizer.fit_on_texts(tokenized_data)\n",
        "\n",
        "    return tokenizer,bpemb\n",
        "\n",
        "# Function to build tokenizer\n",
        "# def build_tokenizer(data, tokenization_method, oov_token, max_vocab_size):\n",
        "#     tokenizer = Tokenizer(oov_token=oov_token, filters='')\n",
        "#     tokenizer.fit_on_texts(data)\n",
        "#     if max_vocab_size:\n",
        "#         tokenizer.word_index = {word: idx for word, idx in tokenizer.word_index.items() if idx <= max_vocab_size}\n",
        "#     return tokenizer\n",
        "\n",
        "def process_pipeline_with_config(\n",
        "    config: Optional[PreprocessingConfig] = None,\n",
        "    vocab_save_dir=\"vocabularies\",\n",
        "    selected_fold: Optional[int] = None  # None means use all folds (k-fold mode)\n",
        "    ):\n",
        "    # Create preprocessor with optional config\n",
        "    preprocessor = ConfigurablePreprocessor(config or PreprocessingConfig())\n",
        "\n",
        "    # Ensure the directory for saving vocabularies exists\n",
        "    Path(vocab_save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load data dynamically based on the configuration\n",
        "    combined_data_final = preprocessor.call_data(None)\n",
        "\n",
        "    # Apply preprocessing with configuration\n",
        "    combined_data_final['label'] = combined_data_final.apply(\n",
        "        lambda x: preprocessor.preprocess_text(x['label'], 'indo'),\n",
        "        axis=1\n",
        "    )\n",
        "    combined_data_final['text'] = combined_data_final.apply(\n",
        "        lambda x: preprocessor.preprocess_text(x['text'], 'java'),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Add start and end tokens\n",
        "    add_start_end_tokens(combined_data_final, 'label')\n",
        "\n",
        "    # Add length information\n",
        "    combined_data_final['length_ind_sentence'] = combined_data_final['label'].apply(lambda x: len(x.split()))\n",
        "    combined_data_final['length_jav_sentence'] = combined_data_final['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Filter data based on configuration parameters\n",
        "    combined_data_final = combined_data_final[\n",
        "        (combined_data_final['length_ind_sentence'] <= config.max_sentence_length) &\n",
        "        (combined_data_final['length_ind_sentence'] > config.min_word_length) &\n",
        "        (combined_data_final['length_jav_sentence'] <= config.max_sentence_length) &\n",
        "        (combined_data_final['length_jav_sentence'] > config.min_word_length)\n",
        "    ]\n",
        "\n",
        "    # Build tokenizers\n",
        "    java_tokenizer,bpemb_jv = build_tokenizer(\n",
        "        combined_data_final['text'], config.tokenization_method, config.oov_token, config.max_vocab_size,lang = 'jv'\n",
        "    )\n",
        "    # print(\"Java Tokenizer\",java_tokenizer.word_index,len(java_tokenizer.word_index))\n",
        "    indo_tokenizer, bpemb_id = build_tokenizer(\n",
        "        combined_data_final['label'], config.tokenization_method, config.oov_token, config.max_vocab_size,lang = 'id'\n",
        "    )\n",
        "    # print(\"indo Tokenizer\",indo_tokenizer.word_index,len(indo_tokenizer.word_index))\n",
        "\n",
        "    # Save vocabularies\n",
        "    with open(f\"{vocab_save_dir}/indonesian_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(indo_tokenizer.word_index, f)\n",
        "    with open(f\"{vocab_save_dir}/javanese_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(java_tokenizer.word_index, f)\n",
        "\n",
        "    # Determine the maximum length for padding\n",
        "    max_length = max(combined_data_final['length_jav_sentence'].max(), combined_data_final['length_ind_sentence'].max())\n",
        "\n",
        "    # Encode and pad the data\n",
        "    X = encode_and_pad(java_tokenizer, combined_data_final['text'], max_length, config.padding_type,lang = 'jv')\n",
        "    y = encode_and_pad(indo_tokenizer, combined_data_final['label'], max_length, config.padding_type,lang = 'id')\n",
        "\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    print(X[1])\n",
        "    print(y[1])\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "        X, y, test_size=config.test_size, random_state=config.random_state\n",
        "    )\n",
        "\n",
        "    # Perform k-fold splitting\n",
        "    kfold = KFold(n_splits=config.k_fold_splits, shuffle=config.k_fold_shuffle, random_state=config.random_state)\n",
        "    fold_splits = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_trainval)):\n",
        "        X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n",
        "        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n",
        "        fold_splits.append((X_train, X_val, y_train, y_val))\n",
        "\n",
        "    # Choose mode: k-fold or specific fold\n",
        "    if selected_fold is None:\n",
        "        # Use all folds\n",
        "        print(\"Training using all folds (k-fold mode).\")\n",
        "        all_folds_data = []\n",
        "        all_test_data = []\n",
        "        for fold_num, (X_train, X_val, y_train, y_val) in enumerate(fold_splits):\n",
        "            all_folds_data.append({\n",
        "                'fold': fold_num,\n",
        "                'X_train': X_train,\n",
        "                'X_val': X_val,\n",
        "                'y_train': y_train,\n",
        "                'y_val': y_val\n",
        "            })\n",
        "        # Save datasets for all folds\n",
        "        with open('dataset_kfolds.pkl', 'wb') as file:\n",
        "            pickle.dump(all_folds_data, file)\n",
        "\n",
        "\n",
        "        all_test_data.append({\n",
        "            'X_test': X_test,\n",
        "            'y_test': y_test\n",
        "        })\n",
        "\n",
        "\n",
        "        with open('dataset_test.pkl', 'wb') as file:\n",
        "            pickle.dump(all_test_data, file)\n",
        "\n",
        "        # Save dataset and tokenizer information\n",
        "        tokenizer_info = {\n",
        "            'max_length': max_length,\n",
        "            'num_encoder_tokens': len(java_tokenizer.word_index) + 1,\n",
        "            'num_decoder_tokens': len(indo_tokenizer.word_index) + 1,\n",
        "            'target_token_index': indo_tokenizer.word_index,\n",
        "            'input_token_index': java_tokenizer.word_index,\n",
        "            'java_tokenizer': java_tokenizer,\n",
        "            'indo_tokenizer': indo_tokenizer,\n",
        "            'bpemb_jv':bpemb_jv,\n",
        "            'bpemb_id':bpemb_id\n",
        "        }\n",
        "        with open('tokenizer_info.pkl', 'wb') as file:\n",
        "            pickle.dump(tokenizer_info, file)\n",
        "\n",
        "         # Print vocab sizes and splits\n",
        "        print(f\"Javanese vocabulary size: {len(java_tokenizer.word_index)}\")\n",
        "        print(f\"Indonesian vocabulary size: {len(indo_tokenizer.word_index)}\")\n",
        "        print(f\"Train set size: {len(X_train)}\")\n",
        "        print(f\"Validation set size: {len(X_val)}\")\n",
        "        print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "        return all_folds_data, tokenizer_info, X_test, y_test\n",
        "\n",
        "    else:\n",
        "        # Use a specific fold\n",
        "        if selected_fold < 0 or selected_fold >= config.k_fold_splits:\n",
        "            raise ValueError(f\"Invalid selected_fold: {selected_fold}. Must be in range 0 to {config.k_fold_splits - 1}.\")\n",
        "\n",
        "        X_train, X_val, y_train, y_val = fold_splits[selected_fold]\n",
        "\n",
        "        trainval_data = {\n",
        "            'X_train': X_train,\n",
        "            'X_val': X_val,\n",
        "            'y_train': y_train,\n",
        "            'y_val': y_val\n",
        "        }\n",
        "\n",
        "        with open('dataset_trainval.pkl','wb') as file:\n",
        "            pickle.dump(trainval_data,file)\n",
        "\n",
        "        print(f\"Training using fold {selected_fold}.\")\n",
        "\n",
        "        # Save dataset and tokenizer information\n",
        "        tokenizer_info = {\n",
        "            'max_length': max_length,\n",
        "            'num_encoder_tokens': len(java_tokenizer.word_index) + 1,\n",
        "            'num_decoder_tokens': len(indo_tokenizer.word_index) + 1,\n",
        "            'target_token_index': indo_tokenizer.word_index,\n",
        "            'input_token_index': java_tokenizer.word_index,\n",
        "            'java_tokenizer': java_tokenizer,\n",
        "            'indo_tokenizer': indo_tokenizer,\n",
        "            'bpemb_jv':bpemb_jv,\n",
        "            'bpemb_id':bpemb_id\n",
        "        }\n",
        "        with open('tokenizer_info.pkl', 'wb') as file:\n",
        "            pickle.dump(tokenizer_info, file)\n",
        "\n",
        "        print(f\"Javanese vocabulary size: {len(java_tokenizer.word_index)}\")\n",
        "        print(f\"Indonesian vocabulary size: {len(indo_tokenizer.word_index)}\")\n",
        "        print(f\"Train set size: {len(X_train)}\")\n",
        "        print(f\"Validation set size: {len(X_val)}\")\n",
        "        print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "        return tokenizer_info,X_train, X_val, y_train, y_val, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2wgKgd2TE1z"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-A4bV-TTFoQ",
        "outputId": "8b30fd6e-cc31-4e03-b8d7-73b37771b1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved to custom_preprocessing_config.yaml\n",
            "datasets has been combined, going to save as csv with name of combined_data_final.csv\n",
            "downloading https://nlp.h-its.org/bpemb/jv/jv.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 393866/393866 [00:00<00:00, 15764953.76B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/jv/jv.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3779806/3779806 [00:00<00:00, 43154816.37B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 396303/396303 [00:00<00:00, 12108123.18B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 3780176/3780176 [00:00<00:00, 36928397.15B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (14962, 35)\n",
            "y shape: (14962, 35)\n",
            "[ 529   12   22   34  980  216   58   55   34  932    2    6 1410   28\n",
            "   55   68  320  307   23   14   88  317   36  304  163  290   35  164\n",
            "  328  800 4060    0    0    0    0]\n",
            "[   2    3    6    7    4  449   14   24   26 2315  109   89   17   26\n",
            "  811    8   11  112 2316   17   73  712   10   73 3607  347    9    2\n",
            "    3    5    4    0    0    0    0]\n",
            "Training using all folds (k-fold mode).\n",
            "Javanese vocabulary size: 5981\n",
            "Indonesian vocabulary size: 6387\n",
            "Train set size: 9576\n",
            "Validation set size: 2393\n",
            "Test set size: 2993\n",
            "Pipeline processed for all folds (k-fold mode) and results saved.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from dataclasses import asdict\n",
        "\n",
        "\n",
        "\n",
        "def save_config_to_yaml(config: PreprocessingConfig, file_path: str):\n",
        "    \"\"\"\n",
        "    Save the preprocessing configuration to a YAML file.\n",
        "\n",
        "    Args:\n",
        "        config (PreprocessingConfig): The configuration object.\n",
        "        file_path (str): Path to the YAML file.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"w\") as yaml_file:\n",
        "        yaml.dump(asdict(config), yaml_file, default_flow_style=False)\n",
        "    print(f\"Configuration saved to {file_path}\")\n",
        "\n",
        "def load_config_from_yaml(file_path: str) -> PreprocessingConfig:\n",
        "    \"\"\"\n",
        "    Load a preprocessing configuration from a YAML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the YAML file.\n",
        "\n",
        "    Returns:\n",
        "        PreprocessingConfig: The loaded configuration object.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as yaml_file:\n",
        "        config_dict = yaml.safe_load(yaml_file)\n",
        "    return PreprocessingConfig(**config_dict)\n",
        "\n",
        "def main():\n",
        "    # Set selected fold: choose specific fold or None for k-fold\n",
        "    selected_fold = None\n",
        "\n",
        "    # Define a custom configuration\n",
        "    custom_config = PreprocessingConfig(\n",
        "        #Preprocessing\n",
        "        lowercase=True,\n",
        "        remove_urls=True,\n",
        "        remove_punctuation=True,\n",
        "        remove_html_tags=True,\n",
        "        remove_emails=True,\n",
        "        remove_quotes=True,\n",
        "        #Tokenizer tuning\n",
        "        min_word_length=3,\n",
        "        enable_slang_replacement=True,\n",
        "        max_sentence_length=35,\n",
        "        max_vocab_size=30000,\n",
        "        min_word_frequency=2,\n",
        "        #Split and Folds\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        k_fold_splits=5,\n",
        "\n",
        "        k_fold_shuffle=True,\n",
        "        #Dataset\n",
        "        data_nusa_writes = True,\n",
        "        data_nusax = False,\n",
        "        data_korpus_nusantara = False,\n",
        "        data_final = False\n",
        "\n",
        "    )\n",
        "\n",
        "    # Save the configuration as YAML\n",
        "    config_file = \"custom_preprocessing_config.yaml\"\n",
        "    save_config_to_yaml(custom_config, config_file)\n",
        "\n",
        "    # Load the configuration from YAML\n",
        "    loaded_config = load_config_from_yaml(config_file)\n",
        "\n",
        "    # Process the pipeline with the loaded configuration\n",
        "    if selected_fold is None:\n",
        "        all_folds_data, tokenizer_info, X_test, y_test = process_pipeline_with_config(config=loaded_config)\n",
        "        # Save the pipeline results (optional)\n",
        "        with open(\"pipeline_results.pkl\", \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"fold_splits\": all_folds_data,\n",
        "                \"tokenizer_info\": tokenizer_info,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_test\": y_test,\n",
        "            }, f)\n",
        "        print(\"Pipeline processed for all folds (k-fold mode) and results saved.\")\n",
        "    else:\n",
        "        if selected_fold < 0 or selected_fold >= custom_config.k_fold_splits:\n",
        "            raise ValueError(f\"Invalid selected_fold: {selected_fold}. Must be in range 0 to {custom_config.k_fold_splits - 1}.\")\n",
        "\n",
        "        tokenizer_info, X_train, X_val, y_train, y_val, X_test, y_test = process_pipeline_with_config(\n",
        "            config=loaded_config, selected_fold=selected_fold\n",
        "        )\n",
        "        # Save the pipeline results (optional)\n",
        "        with open(f\"pipeline_results-selected_fold_{selected_fold}.pkl\", \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"tokenizer_info\": tokenizer_info,\n",
        "                \"X_train\": X_train,\n",
        "                \"X_val\": X_val,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_val\": y_val,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_test\": y_test,\n",
        "                'bpemb_jv':bpemb_jv,\n",
        "                'bpemb_id':bpemb_id\n",
        "            }, f)\n",
        "        print(f\"Pipeline processed for selected fold {selected_fold} and results saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eJV920b7aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346a7b4c-7f1f-41b9-c2e5-468c91d49b19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-ff837e7bbfa2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset = {\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0;34m'X_test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0;34m'y_test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         }\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ],
      "source": [
        "test_dataset = {\n",
        "            'X_test': X_test,\n",
        "            'y_test': y_test\n",
        "        }\n",
        "\n",
        "with open('dataset_test.pkl','wb') as file:\n",
        "    pickle.dump(test_dataset,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw58jKY0d9X3"
      },
      "source": [
        "## Functions to Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yUl_Jstd3F-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(path):\n",
        "    return pd.read_json(path).drop(columns='id')\n",
        "\n",
        "def load_dualisme_java_dictionary():\n",
        "    # Load slang dictionary from CSV\n",
        "    try:\n",
        "      dualisme_dict = pd.read_csv(\"/content/drive/MyDrive/Dataset/MT-JavaIndo/nusa/dict_dualisme.csv\")\n",
        "      return dict(zip(dualisme_dict['dualisme'], dualisme_dict['usedword']))\n",
        "    except FileNotFoundError as e:\n",
        "      print(f\"Warning: Slang dictionary file not found. Proceeding without slang replacement. Error: {e}\")\n",
        "      return {}\n",
        "\n",
        "# Function to load slang dictionary\n",
        "def load_slang_dictionary():\n",
        "    # Load slang dictionary from CSV\n",
        "    try:\n",
        "        slang_dict_1 = pd.read_csv(\n",
        "            \"/content/drive/MyDrive/Dataset/MT-JavaIndo/new_kamusalay.csv\",\n",
        "            encoding=\"latin1\",\n",
        "            names=[\"slang\", \"formal\"]  # Set custom column names\n",
        "        )\n",
        "        slang_dict_2 = pd.read_csv(\n",
        "            \"/content/drive/MyDrive/Dataset/MT-JavaIndo/inforformal-formal-Indonesian-dictionary.tsv\",\n",
        "            sep='\\t', header=0\n",
        "        )\n",
        "        slang_dict_2 = slang_dict_2.rename(columns={'informal': 'slang'})\n",
        "        slang_dict_3 = pd.read_csv('/content/drive/MyDrive/Dataset/MT-JavaIndo/dataset cerpen/kamus_alay_versi2.csv')\n",
        "        slang_dict_3 = slang_dict_3.rename(columns={'Word': 'slang', 'formal word': 'formal'})\n",
        "\n",
        "        # Combine the slang dictionaries\n",
        "        slang_dict = pd.concat([slang_dict_1, slang_dict_2, slang_dict_3], ignore_index=True)\n",
        "        return dict(zip(slang_dict['slang'], slang_dict['formal']))\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Warning: Slang dictionary file not found. Proceeding without slang replacement. Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Preprocessing the text\n",
        "def preprocess_text(text,label):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if input is not valid\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "      text = str(text)\n",
        "\n",
        "    # Load slang dictionary\n",
        "    if label == 'indo':\n",
        "      slang_dict = load_slang_dictionary()\n",
        "    elif label == 'java':\n",
        "      slang_dict = load_dualisme_java_dictionary()\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation, URLs, HTML tags, emails\n",
        "    text = re.sub(r'[%.,?!\":;()[]\"-=@©Ã$¨*‰]', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove quotes\n",
        "    text = text.strip('\"').strip('“').strip('”')\n",
        "\n",
        "    # # Handle word repetition\n",
        "    # text = handle_word_repetition(text)\n",
        "\n",
        "    # Replace slang words using the slang dictionary\n",
        "    words = text.split()\n",
        "    words = [\n",
        "        slang_dict.get(word, slang_dict.get(word, word)) for word in words if word\n",
        "    ]\n",
        "\n",
        "    # Join words back into text\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    # Remove special characters, punctuation, and extra whitespace\n",
        "    text = re.sub(r\"[^\\w\\s-]\", \"\", text)\n",
        "    text = \"\".join([char for char in text if char not in punctuation or char == '-'])\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # Remove single characters\n",
        "    text = ' '.join([w for w in text.split() if len(w) > 1])\n",
        "\n",
        "    # Normalize and remove diacritics\n",
        "    normalized_text = unicodedata.normalize('NFKD', text)\n",
        "    text = ''.join([c for c in normalized_text if not unicodedata.combining(c)])\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize_combined_data_1(text):\n",
        "      tokens = word_tokenize(text)\n",
        "\n",
        "      # Join the tokens back into a string\n",
        "      preprocessed_text = \" \".join(tokens)\n",
        "\n",
        "      return preprocessed_text\n",
        "\n",
        "\n",
        "    # Function to add start and end tokens to labels\n",
        "def add_start_end_tokens(data, column):\n",
        "    data[column] = data[column].apply(lambda x: f'<start> {x} <end>')\n",
        "\n",
        "# Function to calculate vocabulary size\n",
        "def get_vocab(data, column):\n",
        "    vocab = defaultdict(list)\n",
        "    for index, sentence in enumerate(data[column]):\n",
        "        for word in sentence.split():\n",
        "            vocab[word].append(sentence)  # Append the sentence to the word's list\n",
        "    return vocab\n",
        "\n",
        "# Function to encode and pad sequences\n",
        "def encode_and_pad(tokenizer, data, length):\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "    return pad_sequences(sequences, maxlen=length, padding='post')\n",
        "\n",
        "# Function to build tokenizer\n",
        "# def build_tokenizer(data):\n",
        "#     tokenizer = Tokenizer(oov_token='<OOV>', filters='')\n",
        "#     tokenizer.fit_on_texts(data)\n",
        "#     return tokenizer\n",
        "\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def build_tokenizer(data, lang='jv', vocab_size=20000):\n",
        "    # Load BPEmb Javanese model\n",
        "    bpemb = BPEmb(lang=lang, vs=vocab_size)\n",
        "\n",
        "    # Tokenize data using BPEmb\n",
        "    tokenized_data = [' '.join(bpemb.encode(text)) for text in data]\n",
        "\n",
        "    # Create Keras Tokenizer\n",
        "    tokenizer = Tokenizer(oov_token='<OOV>', filters='')\n",
        "    tokenizer.fit_on_texts(tokenized_data)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def preprocess_final_data(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Load slang dictionary\n",
        "    try:\n",
        "        slang_dict = load_slang_dictionary()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: Slang dictionary file not found. Proceeding without slang replacement.\")\n",
        "        slang_dict = {}\n",
        "\n",
        "    slang_dict = {key: str(value) for key, value in slang_dict.items()}\n",
        "    words = text.split()\n",
        "    words = [slang_dict.get(word, \"\") if slang_dict.get(word) == \"null\" else slang_dict.get(word,word) for word in words if word]\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASsQ3lWdEqtk"
      },
      "outputs": [],
      "source": [
        "# check\n",
        "sample_text = \"hati2 hati! www.example.com\"\n",
        "processed_text = preprocess_text(sample_text,'indo')\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqHKZ2E4BHnd"
      },
      "source": [
        "### Call Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrL0T2cveBZJ"
      },
      "source": [
        "## Preprocess Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-zyKEH7eDjy"
      },
      "outputs": [],
      "source": [
        "# Main processing pipeline\n",
        "def process_pipeline(n_splits, random_state,vocab_save_dir=\"vocabularies\"):\n",
        "    Path(vocab_save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    combined_data_final = combined_data_new.copy()\n",
        "    combined_data_final['label'] = combined_data_final.apply(lambda x: preprocess_text(x['label'], 'indo'), axis=1)\n",
        "    combined_data_final['text'] = combined_data_final.apply(lambda x: preprocess_text(x['text'], 'java'), axis=1)\n",
        "    # combined_data_final.to_csv(\"/content/drive/MyDrive/Dataset/csv-aksarajawa/dataset cerpen/combined_data_final_3.csv\")\n",
        "    add_start_end_tokens(combined_data_final,'label')\n",
        "    combined_data_final['length_ind_sentence'] = combined_data_final['label'].apply(lambda x: len(x.split()))\n",
        "    combined_data_final['length_jav_sentence'] = combined_data_final['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    combined_data_final = combined_data_final[\n",
        "        (combined_data_final['length_ind_sentence'] <= 35) &\n",
        "        (combined_data_final['length_ind_sentence'] > 2) &\n",
        "        (combined_data_final['length_jav_sentence'] <= 35) &\n",
        "        (combined_data_final['length_jav_sentence'] > 2)\n",
        "    ]\n",
        "    ind_vocab = get_vocab(combined_data_final, 'label')\n",
        "    jav_vocab = get_vocab(combined_data_final, 'text')\n",
        "    # Build tokenizers\n",
        "    java_tokenizer = build_tokenizer(combined_data_final['text'])\n",
        "    indo_tokenizer = build_tokenizer(combined_data_final['label'])\n",
        "\n",
        "    print(f\"Saving vocabularies to {vocab_save_dir}...\")\n",
        "    with open(f\"{vocab_save_dir}/indonesian_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(ind_vocab, f)\n",
        "    with open(f\"{vocab_save_dir}/javanese_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(jav_vocab, f)\n",
        "\n",
        "    print(\"Vocabularies saved successfully\")\n",
        "\n",
        "    input_token_index = java_tokenizer.word_index  # Javanese token index\n",
        "    target_token_index = indo_tokenizer.word_index  # Indonesian token index\n",
        "\n",
        "    max_length = max(combined_data_final['length_jav_sentence'].max(), combined_data_final['length_ind_sentence'].max())\n",
        "\n",
        "    # Encode sequences\n",
        "    X = encode_and_pad(java_tokenizer, combined_data_final['text'], max_length)\n",
        "    y = encode_and_pad(indo_tokenizer, combined_data_final['label'], max_length)\n",
        "\n",
        "    X_trainval,X_test,y_trainval,y_test = train_test_split(X,y,test_size = 0.1, random_state = 42)\n",
        "\n",
        "    test_data = {\n",
        "    'X_test': X_test,\n",
        "    'y_test': y_test\n",
        "    }\n",
        "\n",
        "    # Save both X_test and y_test in a single pickle file\n",
        "    with open('test_data.pkl', 'wb') as f:\n",
        "        pickle.dump(test_data, f)\n",
        "\n",
        "    print(\"X_test and y_test have been saved in test_data.pkl\")\n",
        "\n",
        "    num_encoder_tokens = len(java_tokenizer.word_index) + 1  # Include OOV token\n",
        "    java_length = max_length\n",
        "    print('Java Vocabulary Size: %d' % num_encoder_tokens)\n",
        "\n",
        "    # Prepare Indonesian tokenizer\n",
        "    num_decoder_tokens = len(indo_tokenizer.word_index) + 1  # Include OOV token\n",
        "    indo_length = max_length\n",
        "    print('Indonesian Vocabulary Size: %d' % num_decoder_tokens)\n",
        "\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    fold_splits = []\n",
        "\n",
        "    # Perform k-fold splitting\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_trainval)):\n",
        "        print(f'\\nFold {fold + 1}:')\n",
        "        print(f'Training samples: {len(train_idx)}, Testing samples: {len(val_idx)}')\n",
        "\n",
        "        X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n",
        "        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n",
        "\n",
        "        fold_splits.append((X_train, X_val, y_train, y_val))\n",
        "    X_train,X_val,y_train,y_val = fold_splits[3]\n",
        "    # Create info dictionary\n",
        "    tokenizer_info = {\n",
        "        'max_length': max_length,\n",
        "        'num_encoder_tokens': num_encoder_tokens,\n",
        "        'num_decoder_tokens': num_decoder_tokens,\n",
        "        'target_token_index': indo_tokenizer.word_index,\n",
        "        'input_token_index': java_tokenizer.word_index,\n",
        "        'java_tokenizer': java_tokenizer,\n",
        "        'indo_tokenizer': indo_tokenizer\n",
        "    }\n",
        "\n",
        "    with open('dataset_kfolds.pkl','wb') as file:\n",
        "      pickle.dump(fold_splits,file)\n",
        "    with open('tokenizer_info.pkl','wb') as file:\n",
        "      pickle.dump(tokenizer_info,file)\n",
        "\n",
        "    print(\"Preprocessing Done!\")\n",
        "\n",
        "    return fold_splits, tokenizer_info,X_test,y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60RwPngHhz8u"
      },
      "outputs": [],
      "source": [
        "def is_potential_non_normalized(word: str) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a word potentially needs normalization\n",
        "\n",
        "    Args:\n",
        "        word (str): Word to check\n",
        "\n",
        "    Returns:\n",
        "        bool: True if word might need normalization\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        r'(.)\\1{2,}',  # Three or more repeated characters\n",
        "        r'\\d+',        # Contains numbers\n",
        "        r'[A-Z]+',     # Contains uppercase (shouldn't exist after preprocessing)\n",
        "        r'[!@#$%^&*(),.?\":{}|<>]+',  # Contains special characters\n",
        "        r'(.+?)\\1{1,}',  # Repeated patterns\n",
        "        r'2\\b',        # Ends with '2' (common in Indonesian informal writing)\n",
        "        r'[aiueo]{3,}'  # Three or more consecutive vowels\n",
        "    ]\n",
        "\n",
        "    return any(re.search(pattern, word) for pattern in patterns)\n",
        "\n",
        "def find_non_normalized_words(vocab: Dict) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Find potentially non-normalized words in vocabulary\n",
        "\n",
        "    Args:\n",
        "        vocab (dict): Vocabulary dictionary\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of suspicious patterns and their matching words\n",
        "    \"\"\"\n",
        "    suspicious_words = defaultdict(list)\n",
        "\n",
        "    for word in vocab:\n",
        "        # Check for repeated characters\n",
        "        if re.search(r'(.)\\1{2,}', word):\n",
        "            suspicious_words['repeated_chars'].append(word)\n",
        "\n",
        "        # Check for numbers\n",
        "        if re.search(r'\\d', word):\n",
        "            suspicious_words['contains_numbers'].append(word)\n",
        "\n",
        "        # Check for special characters\n",
        "        if re.search(r'[^a-z\\s-]', word):\n",
        "            suspicious_words['special_chars'].append(word)\n",
        "\n",
        "        # Check for very long words (possibly not properly split)\n",
        "        if len(word) > 20:\n",
        "            suspicious_words['very_long'].append(word)\n",
        "\n",
        "        # Check for repeated patterns\n",
        "        if re.search(r'(.+?)\\1{1,}', word):\n",
        "            suspicious_words['repeated_patterns'].append(word)\n",
        "\n",
        "        # Check for multiple consecutive vowels\n",
        "        if re.search(r'[aiueo]{3,}', word):\n",
        "            suspicious_words['multiple_vowels'].append(word)\n",
        "\n",
        "    return suspicious_words\n",
        "\n",
        "def export_vocabularies(ind_vocab: Dict, jav_vocab: Dict, output_dir: str = \"vocab_analysis\") -> None:\n",
        "    # Create output directory\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save raw vocabularies as pickle\n",
        "    with open(f\"{output_dir}/indonesian_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(ind_vocab, f)\n",
        "    with open(f\"{output_dir}/javanese_vocab.pkl\", 'wb') as f:\n",
        "        pickle.dump(jav_vocab, f)\n",
        "\n",
        "    # Find potentially non-normalized words\n",
        "    ind_suspicious = find_non_normalized_words(ind_vocab)\n",
        "    jav_suspicious = find_non_normalized_words(jav_vocab)\n",
        "\n",
        "    # Create detailed CSV with all words and their analysis\n",
        "    with open(f\"{output_dir}/vocabulary_analysis.csv\", 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Language', 'Word', 'Length', 'Suspicious_Patterns', 'Needs_Review', 'Source_Sentences'])\n",
        "\n",
        "        # Process Indonesian vocabulary\n",
        "        for word, sentences in ind_vocab.items():\n",
        "            patterns = []\n",
        "            if re.search(r'(.)\\1{2,}', word): patterns.append('repeated_chars')\n",
        "            if re.search(r'\\d', word): patterns.append('contains_numbers')\n",
        "            if re.search(r'[^a-z\\s-]', word): patterns.append('special_chars')\n",
        "            if len(word) > 20: patterns.append('very_long')\n",
        "            if re.search(r'(.+?)\\1{1,}', word): patterns.append('repeated_patterns')\n",
        "            if re.search(r'[aiueo]{3,}', word): patterns.append('multiple_vowels')\n",
        "\n",
        "            needs_review = len(patterns) > 0\n",
        "            writer.writerow(['Indonesian', word, len(word),\n",
        "                             '|'.join(patterns) if patterns else 'none',\n",
        "                             'YES' if needs_review else 'NO',\n",
        "                             '; '.join(sentences)])  # Join sentences for the word\n",
        "\n",
        "        # Process Javanese vocabulary\n",
        "        for word, sentences in jav_vocab.items():\n",
        "            patterns = []\n",
        "            if re.search(r'(.)\\1{2,}', word): patterns.append('repeated_chars')\n",
        "            if re.search(r'\\d', word): patterns.append('contains_numbers')\n",
        "            if re.search(r'[^a-z\\s-]', word): patterns.append('special_chars')\n",
        "            if len(word) > 20: patterns.append('very_long')\n",
        "            if re.search(r'(.+?)\\1{1,}', word): patterns.append('repeated_patterns')\n",
        "            if re.search(r'[aiueo]{3,}', word): patterns.append('multiple_vowels')\n",
        "\n",
        "            needs_review = len(patterns) > 0\n",
        "            writer.writerow(['Javanese', word, len(word),\n",
        "                             '|'.join(patterns) if patterns else 'none',\n",
        "                             'YES' if needs_review else 'NO',\n",
        "                             '; '.join(sentences)])  # Join sentences for the word\n",
        "\n",
        "    # Create summary report\n",
        "    with open(f\"{output_dir}/normalization_summary.txt\", 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Vocabulary Analysis Summary\\n\")\n",
        "        f.write(\"=========================\\n\\n\")\n",
        "\n",
        "        f.write(\"Indonesian Vocabulary:\\n\")\n",
        "        f.write(f\"Total words: {len(ind_vocab)}\\n\")\n",
        "        f.write(\"Potentially non-normalized words:\\n\")\n",
        "        for pattern, words in ind_suspicious.items():\n",
        "            f.write(f\"\\n{pattern}: {len(words)} words\\n\")\n",
        "            f.write(\"Sample words: \" + \", \".join(words[:10]) + \"\\n\")\n",
        "\n",
        "        f.write(\"\\nJavanese Vocabulary:\\n\")\n",
        "        f.write(f\"Total words: {len(jav_vocab)}\\n\")\n",
        "        f.write(\"Potentially non-normalized words:\\n\")\n",
        "        for pattern, words in jav_suspicious.items():\n",
        "            f.write(f\"\\n{pattern}: {len(words)} words\\n\")\n",
        "            f.write(\"Sample words: \" + \", \".join(words[:10]) + \"\\n\")\n",
        "\n",
        "def analyze_vocabularies(vocab_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Load and analyze saved vocabularies\n",
        "\n",
        "    Args:\n",
        "        vocab_dir (str): Directory containing vocabulary pickle files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load vocabularies\n",
        "        with open(f\"{vocab_dir}/indonesian_vocab.pkl\", 'rb') as f:\n",
        "            ind_vocab = pickle.load(f)\n",
        "        with open(f\"{vocab_dir}/javanese_vocab.pkl\", 'rb') as f:\n",
        "            jav_vocab = pickle.load(f)\n",
        "\n",
        "        # Export and analyze\n",
        "        export_vocabularies(ind_vocab, jav_vocab, f\"{vocab_dir}/analysis\")\n",
        "\n",
        "        print(f\"\\nAnalysis completed. Check the '{vocab_dir}/analysis' directory for:\")\n",
        "        print(\"- vocabulary_analysis.csv: Detailed analysis of each word\")\n",
        "        print(\"- normalization_summary.txt: Summary of potentially non-normalized words\")\n",
        "        print(\"- Raw vocabularies in pickle format\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No vocabulary files found in {vocab_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing vocabulary: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jJWrXykCQxd"
      },
      "outputs": [],
      "source": [
        "def analyze_text_distribution(dataframe, text_column):\n",
        "    # Calculate text lengths\n",
        "    dataframe['text_length'] = dataframe[text_column].str.len()\n",
        "\n",
        "    # Plot text length distribution\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Subplot 1: Histogram of text lengths\n",
        "    plt.subplot(131)\n",
        "    dataframe['text_length'].hist(bins=50)\n",
        "    plt.title('Text Length Distribution')\n",
        "    plt.xlabel('Length')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Subplot 2: Box plot of text lengths\n",
        "    plt.subplot(132)\n",
        "    sns.boxplot(x=dataframe['text_length'])\n",
        "    plt.title('Text Length Boxplot')\n",
        "\n",
        "    # Subplot 3: Descriptive statistics\n",
        "    plt.subplot(133)\n",
        "    length_stats = dataframe['text_length'].describe()\n",
        "    plt.text(0.5, 0.5, str(length_stats),\n",
        "             horizontalalignment='center',\n",
        "             verticalalignment='center')\n",
        "    plt.title('Length Statistics')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_vocabulary(dataframe, text_column):\n",
        "    # Tokenize and count unique words\n",
        "    all_words = ' '.join(dataframe[text_column]).split()\n",
        "    word_freq = pd.Series(all_words).value_counts()\n",
        "\n",
        "    # Plot word frequency\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Top 50 most frequent words\n",
        "    plt.subplot(121)\n",
        "    word_freq[:50].plot(kind='bar')\n",
        "    plt.title('Top 50 Most Frequent Words')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    # Word frequency distribution\n",
        "    plt.subplot(122)\n",
        "    word_freq.plot(kind='hist', bins=50, log=True)\n",
        "    plt.title('Word Frequency Distribution (Log Scale)')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Number of Words')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'total_unique_words': len(word_freq),\n",
        "        'top_10_words': word_freq[:10]\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWj0FDKCebCX"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlJ4VXbeeS9"
      },
      "outputs": [],
      "source": [
        "fold_splits, tokenizer_info,X_test,y_test = process_pipeline(n_splits = 5, random_state = 42)\n",
        "# X_train, X_test, y_train, y_test, max_length,num_encoder_tokens,num_decoder_tokens,target_token_index,input_token_index,java_tokenizer,indo_tokenizer = process_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NygmYzqgjMdB"
      },
      "outputs": [],
      "source": [
        "analyze_vocabularies(\"/content/vocabularies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rX-vZqeCR20"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer_info.pkl','rb') as file:\n",
        "  tokenizer_info = pickle.load(file)\n",
        "\n",
        "with open('dataset_kfolds.pkl','rb') as file:\n",
        "  fold_splits = pickle.load(file)\n",
        "\n",
        "fold_splits[3]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LqM5V3k3P_yo",
        "nVnoPvcCRXf-",
        "ND5sE7NFPUCk",
        "Dhrqx1lTPWqa",
        "SpT2SB0uibOY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}