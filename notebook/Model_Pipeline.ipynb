{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JZ4kcezY6NwX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ30aCxMfQsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cd0139-2523-46a9-b7cc-d0667072bdf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install neptune -q\n",
        "!pip install bpemb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZA1rTANthes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a61c55-3754-4810-cc75-15bf6a35e611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
          ]
        }
      ],
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "import pickle\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Optional, Union\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "import dataclasses\n",
        "import time\n",
        "\n",
        "\n",
        "# Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Text and Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from string import punctuation, digits\n",
        "\n",
        "# Machine Learning Utilities\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Dense, Bidirectional, Concatenate,\n",
        "    LayerNormalization, ActivityRegularization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.regularizers import l1_l2, l2\n",
        "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "\n",
        "# Neptune.ai for Experiment Tracking\n",
        "import neptune.new as neptune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9bcyUQB8Mll",
        "outputId": "16a0a930-46cf-4752-be62-4838c7890e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQGoGcs-Caqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "o2RMhjcR6QUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class ModelConfig:\n",
        "    # Model Architecture Parameters\n",
        "    hidden_units: int = 32\n",
        "    use_bidirectional: bool = True\n",
        "    use_attention: bool = True\n",
        "\n",
        "    # Regularization Parameters\n",
        "    dropout_rate: Optional[float] = 0.3\n",
        "    recurrent_dropout_rate: Optional[float] = 0.1\n",
        "    l1_reg: Optional[float] = 1e-3\n",
        "    l2_reg: Optional[float] = 1e-3\n",
        "\n",
        "    # Training Parameters\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 50\n",
        "    learning_rate: float = 1e-3\n",
        "\n",
        "    # K-Fold Cross Validation Parameters\n",
        "    n_splits: int = 5\n",
        "    selected_folds: Optional[Union[List[int], int]] = None\n",
        "\n",
        "    # Dropout and Regularization Toggles\n",
        "    use_dropout: bool = True\n",
        "    use_recurrent_dropout: bool = True\n",
        "    use_l1_regularization: bool = False\n",
        "    use_l2_regularization: bool = False\n",
        "\n",
        "    # Neptune Tracking Parameters\n",
        "    neptune_project: Optional[str] = None\n",
        "    neptune_api_token: Optional[str] = None\n",
        "    neptune_run_name: Optional[str] = None\n",
        "    neptune_tags: Optional[List[str]] = None\n",
        "\n",
        "    # Versioning and Tracking\n",
        "    version: Optional[str] = None\n",
        "    experiment_id: Optional[str] = None\n",
        "    config_filepath: Optional[str] = None\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Validate selected_folds\n",
        "        if isinstance(self.selected_folds, int):\n",
        "            self.selected_folds = [self.selected_folds]\n",
        "\n",
        "        # Adjust dropout and regularization based on toggle switches\n",
        "        if not self.use_dropout:\n",
        "            self.dropout_rate = None\n",
        "        if not self.use_recurrent_dropout:\n",
        "            self.recurrent_dropout_rate = None\n",
        "        if not self.use_l1_regularization:\n",
        "            self.l1_reg = None\n",
        "        if not self.use_l2_regularization:\n",
        "            self.l2_reg = None\n",
        "\n",
        "        # Set default Neptune parameters if not provided\n",
        "        if not self.neptune_project:\n",
        "            self.neptune_project = \"ihsani.yulfa/Translation-Project\"\n",
        "        if not self.neptune_api_token:\n",
        "            self.neptune_api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmMTAzYmRjZC01YjBlLTRhNDktOTZjYy00MDY4ODdkMzNjZTAifQ==\"\n",
        "        if not self.neptune_run_name:\n",
        "            self.neptune_run_name = f\"Run-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        if not self.neptune_tags:\n",
        "            self.neptune_tags = [\"default\"]\n",
        "\n",
        "        # Set version and experiment ID if not provided\n",
        "        if not self.version:\n",
        "            self.version = f\"v{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        if not self.experiment_id:\n",
        "            self.experiment_id = str(uuid.uuid4())\n",
        "\n",
        "    def save_config(self, output_dir='experiments'):\n",
        "        \"\"\"\n",
        "        Save configuration details to a JSON file\n",
        "\n",
        "        Args:\n",
        "            output_dir (str): Directory to save configuration files\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved configuration file\n",
        "        \"\"\"\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Create a dictionary of configuration details\n",
        "        config_dict = dataclasses.asdict(self)\n",
        "\n",
        "        # Add additional metadata\n",
        "        config_dict['timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "        # Determine filename based on version and experiment ID\n",
        "        filename = f\"config_{self.version}_{self.experiment_id[:8]}.json\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Save configuration to JSON\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=4)\n",
        "\n",
        "        print(f\"Configuration saved to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    @classmethod\n",
        "    def load_config(cls, filepath):\n",
        "        \"\"\"\n",
        "        Load configuration from a JSON file\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the configuration JSON file\n",
        "\n",
        "        Returns:\n",
        "            ModelConfig: Loaded configuration instance\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "\n",
        "        # Remove timestamp and other non-init fields\n",
        "        config_dict.pop('timestamp', None)\n",
        "        config_dict.pop('version', None)\n",
        "        config_dict.pop('experiment_id', None)\n",
        "\n",
        "        return cls(**config_dict)"
      ],
      "metadata": {
        "id": "n7VHl5ne6VU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Decoder"
      ],
      "metadata": {
        "id": "JPj0zZOl6Ycl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "-2r1d8Fk6noh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention\n",
        "# https://colab.research.google.com/drive/1XrjPL3O_szhahYZW0z9yhCl9qvIcJJYW\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate,Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "fPS_5fds6pOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "5q_BKxAd6qdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_encoder_tokens, num_decoder_tokens, max_length,\n",
        "                 hidden_units, use_bidirectional, use_attention,\n",
        "                 dropout_rate, recurrent_dropout_rate,\n",
        "                 l1_reg, l2_reg):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_length,))\n",
        "\n",
        "    # Embedding with regularization\n",
        "    enc_emb = Embedding(\n",
        "        input_dim=num_encoder_tokens,\n",
        "        output_dim=hidden_units\n",
        "    )(encoder_inputs)\n",
        "\n",
        "    # Apply dropout after embedding if specified\n",
        "    # if dropout_rate is not None:\n",
        "    #     enc_emb = tf.keras.layers.Dropout(rate=dropout_rate)(enc_emb)\n",
        "\n",
        "    if use_bidirectional:\n",
        "        # Bidirectional LSTM with regularization\n",
        "        encoder = Bidirectional(\n",
        "            LSTM(\n",
        "                hidden_units,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=dropout_rate if dropout_rate else 0.0,\n",
        "                # recurrent_dropout=recurrent_dropout_rate if recurrent_dropout_rate else 0.0,\n",
        "                # kernel_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "                # recurrent_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "                # bias_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0)\n",
        "            )\n",
        "        )\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(enc_emb)\n",
        "        state_h = Concatenate()([forward_h, backward_h])\n",
        "        state_c = Concatenate()([forward_c, backward_c])\n",
        "        encoder_states = [state_h, state_c]\n",
        "    else:\n",
        "        # Unidirectional LSTM with regularization\n",
        "        encoder = LSTM(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=dropout_rate if dropout_rate else 0.0,\n",
        "            # recurrent_dropout=recurrent_dropout_rate if recurrent_dropout_rate else 0.0,\n",
        "            # kernel_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "            # recurrent_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "            # bias_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0)\n",
        "        )\n",
        "        encoder_outputs, state_h, state_c = encoder(enc_emb)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_emb = Embedding(\n",
        "        input_dim=num_decoder_tokens,\n",
        "        output_dim=hidden_units\n",
        "    )(decoder_inputs)\n",
        "\n",
        "    # if dropout_rate is not None:\n",
        "    #     dec_emb = tf.keras.layers.Dropout(rate=dropout_rate)(dec_emb)\n",
        "\n",
        "    decoder_lstm = LSTM(\n",
        "        hidden_units * 2 if use_bidirectional else hidden_units,\n",
        "        return_sequences=True,\n",
        "        return_state=True,\n",
        "        dropout=dropout_rate if dropout_rate else 0.0,\n",
        "        # recurrent_dropout=recurrent_dropout_rate if recurrent_dropout_rate else 0.0,\n",
        "        # kernel_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "        # recurrent_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "        # bias_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0)\n",
        "    )\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "    if use_attention:\n",
        "        # Attention mechanism\n",
        "        attention_layer = AttentionLayer()\n",
        "        attention_result, _ = attention_layer([encoder_outputs, decoder_outputs])\n",
        "        decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_result])\n",
        "        decoder_outputs = Dense(\n",
        "            num_decoder_tokens,\n",
        "            activation='softmax',\n",
        "            # kernel_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "            # bias_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0)\n",
        "        )(decoder_concat_input)\n",
        "    else:\n",
        "        decoder_outputs = Dense(\n",
        "            num_decoder_tokens,\n",
        "            activation='softmax',\n",
        "            # kernel_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0),\n",
        "            # bias_regularizer=l1_l2(l1=l1_reg if l1_reg else 0.0, l2=l2_reg if l2_reg else 0.0)\n",
        "        )(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "elL2M0Y06pqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "TsyrqRon8Fzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions\n"
      ],
      "metadata": {
        "id": "6Tn17hVg97ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeptuneCallback(Callback):\n",
        "    def __init__(self, run, fold):\n",
        "        super().__init__()\n",
        "        self.run = run\n",
        "        self.fold = fold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for metric_name, metric_value in logs.items():\n",
        "            self.run[f\"train/{metric_name}\"].log(metric_value, step=epoch)\n",
        "\n",
        "def initialize_neptune_run(config: ModelConfig, fold: int):\n",
        "    \"\"\"\n",
        "    Initialize a Neptune run for tracking a specific fold.\n",
        "    \"\"\"\n",
        "    run = neptune.init_run(\n",
        "        project=config.neptune_project,  # Pass project name from config\n",
        "        api_token=config.neptune_api_token,  # Securely pass API token from config\n",
        "        name=f\"{config.version}-Bi-LSTM-{config.use_attention}-Fold-{fold}\",\n",
        "        tags=[\n",
        "            f\"Version-{config.version}\",\n",
        "            f\"Fold-{fold}\",\n",
        "            f\"Experiment-{config.experiment_id[:8]}\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Log configuration to Neptune\n",
        "    run[\"parameters\"] = dataclasses.asdict(config)\n",
        "    run[\"parameters/fold\"] = fold\n",
        "    # run[\"config/filepath\"] = config.config_filepath\n",
        "    run[\"config/version\"] = config.version\n",
        "    run[\"config/experiment_id\"] = config.experiment_id\n",
        "\n",
        "    # Upload the configuration file to Neptune\n",
        "    # run[\"config/file\"].upload(config.config_filepath)\n",
        "\n",
        "    return run\n",
        "\n",
        "def prepare_decoder_data(data):\n",
        "    \"\"\"\n",
        "    Prepare decoder input and target sequences.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset for decoding.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of decoder input and target sequences.\n",
        "    \"\"\"\n",
        "    decoder_input = data[:, :-1].reshape(data.shape[0], data.shape[1] - 1, 1)\n",
        "    decoder_target = data[:, 1:].reshape(data.shape[0], data.shape[1] - 1, 1)\n",
        "    return decoder_input, decoder_target\n",
        "\n",
        "def log_model_architecture(model, run, config):\n",
        "    \"\"\"\n",
        "    Logs the model's architecture summary and plot to Neptune.\n",
        "    \"\"\"\n",
        "    # Generate and upload model plot\n",
        "    plot_filename = f\"Bi-LSTM-{config.use_attention}-{config.hidden_units}.png\"\n",
        "    plot_model(model, to_file=plot_filename, show_shapes=True)\n",
        "    run[\"model/plot\"].upload(plot_filename)\n",
        "\n",
        "    # Log model summary\n",
        "    model_summary = []\n",
        "    model.summary(print_fn=lambda x: model_summary.append(x))\n",
        "    run[\"model/summary\"] = \"\\n\".join(model_summary)\n",
        "\n",
        "def prepare_callbacks(run, fold: int):\n",
        "    \"\"\"\n",
        "    Prepare training callbacks including Neptune integration, LR scheduling, and early stopping.\n",
        "    \"\"\"\n",
        "    lr_scheduler = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-5\n",
        "    )\n",
        "    neptune_cbk = NeptuneCallback(run, fold)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return [lr_scheduler, neptune_cbk]\n",
        "\n",
        "def log_final_metrics(run, history):\n",
        "    \"\"\"\n",
        "    Log final training and validation metrics to Neptune.\n",
        "    \"\"\"\n",
        "    run[\"metrics/final_train_loss\"] = history.history['loss'][-1]\n",
        "    run[\"metrics/final_train_accuracy\"] = history.history['accuracy'][-1]\n",
        "    run[\"metrics/final_val_loss\"] = history.history['val_loss'][-1]\n",
        "    run[\"metrics/final_val_accuracy\"] = history.history['val_accuracy'][-1]\n",
        "    run[\"metrics/epochs_trained\"] = len(history.history['loss'])\n",
        "\n",
        "def log_final_metrics(run, history):\n",
        "    \"\"\"\n",
        "    Log final training and validation metrics to Neptune.\n",
        "    \"\"\"\n",
        "    run[\"metrics/final_train_loss\"] = history.history['loss'][-1]\n",
        "    run[\"metrics/final_train_accuracy\"] = history.history['accuracy'][-1]\n",
        "    run[\"metrics/final_val_loss\"] = history.history['val_loss'][-1]\n",
        "    run[\"metrics/final_val_accuracy\"] = history.history['val_accuracy'][-1]\n",
        "    run[\"metrics/epochs_trained\"] = len(history.history['loss'])\n",
        "\n",
        "def plot_learning_curves(history, run):\n",
        "    \"\"\"\n",
        "    Plot and log the learning curves for loss and accuracy.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss curves\n",
        "    ax1.plot(history.history['loss'], label='Train Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy curves\n",
        "    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    ax2.set_title('Model Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    run[\"visualizations/learning_curves\"].upload(neptune.types.File.as_image(fig))\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "def summarize_training(neptune_runs, fold_histories, fold_scores):\n",
        "    \"\"\"\n",
        "    Summarize and log results from all folds.\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining completed! Summary of results:\")\n",
        "    print(f\"Number of folds completed: {len(fold_scores)}\")\n",
        "\n",
        "    avg_val_acc = np.mean([history['val_accuracy'][-1] for history in fold_histories])\n",
        "    std_val_acc = np.std([history['val_accuracy'][-1] for history in fold_histories])\n",
        "\n",
        "    print(f\"Average validation accuracy: {avg_val_acc:.4f}\")\n",
        "    print(f\"Standard deviation: {std_val_acc:.4f}\")\n",
        "\n",
        "    # Log summary to Neptune\n",
        "    summary_run = neptune.init_run(\n",
        "        project=neptune_runs[0][\"sys/project\"].fetch(),\n",
        "        api_token=neptune_runs[0][\"sys/api_token\"].fetch(),\n",
        "        name=\"Summary-Run\"\n",
        "    )\n",
        "    summary_run[\"metrics/mean_val_accuracy\"] = avg_val_acc\n",
        "    summary_run[\"metrics/std_val_accuracy\"] = std_val_acc\n",
        "    summary_run.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "HjkT5gnn9-EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "e9fdQ4oPAW2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_model_with_kfold(config: ModelConfig,\n",
        "                                      fold_splits_path: str,\n",
        "                                      tokenizer_info_path: str):\n",
        "    \"\"\"\n",
        "    Train machine translation model with configurable k-fold cross-validation\n",
        "\n",
        "    Args:\n",
        "        config (ModelConfig): Configuration for model and training\n",
        "        fold_splits_path (str): Path to pickled fold splits\n",
        "        tokenizer_info_path (str): Path to pickled tokenizer information\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing training histories, evaluation scores, and Neptune runs\n",
        "    \"\"\"\n",
        "\n",
        "    # Save configuration before training\n",
        "    config_filepath = config.save_config()\n",
        "\n",
        "    # Load fold splits and tokenizer info\n",
        "    with open(fold_splits_path, 'rb') as file:\n",
        "        fold_splits = pickle.load(file)\n",
        "\n",
        "    with open(tokenizer_info_path, 'rb') as file:\n",
        "        tokenizer_info = pickle.load(file)\n",
        "\n",
        "    # Extract tokenizer information\n",
        "    num_encoder_tokens = tokenizer_info['num_encoder_tokens']\n",
        "    num_decoder_tokens = tokenizer_info['num_decoder_tokens']\n",
        "    max_length = tokenizer_info['max_length']\n",
        "\n",
        "    # Determine which folds to train\n",
        "    train_folds = config.selected_folds or list(range(len(fold_splits)))\n",
        "    if not all(0 <= fold < len(fold_splits) for fold in train_folds):\n",
        "        raise ValueError(\"Invalid fold indices in selected_folds.\")\n",
        "\n",
        "    fold_histories, fold_scores, neptune_runs = [], [], []\n",
        "\n",
        "    for fold in train_folds:\n",
        "        # Unpack training and validation data for the current fold\n",
        "        fold_data = fold_splits[fold]  # Assuming fold_splits is a list of tuples\n",
        "        X_train, X_val, y_train, y_val = fold_data['X_train'], fold_data['X_val'], fold_data['y_train'], fold_data['y_val']\n",
        "\n",
        "        # Debugging: Print fold information\n",
        "        print(f\"Training on fold {fold}:\")\n",
        "        print(f\"  X_train shape: {X_train.shape}\")\n",
        "        print(f\"  X_val shape: {X_val.shape}\")\n",
        "        print(f\"  y_train shape: {y_train.shape}\")\n",
        "        print(f\"  y_val shape: {y_val.shape}\")\n",
        "\n",
        "        # Initialize Neptune run\n",
        "        run = initialize_neptune_run(config, fold)\n",
        "        neptune_runs.append(run)\n",
        "\n",
        "        # Create model\n",
        "        model = create_model(\n",
        "            num_encoder_tokens=num_encoder_tokens,\n",
        "            num_decoder_tokens=num_decoder_tokens,\n",
        "            max_length=max_length,\n",
        "            hidden_units=config.hidden_units,\n",
        "            use_bidirectional=config.use_bidirectional,\n",
        "            use_attention=config.use_attention,\n",
        "            dropout_rate=config.dropout_rate,\n",
        "            recurrent_dropout_rate=config.recurrent_dropout_rate,\n",
        "            l1_reg=config.l1_reg,\n",
        "            l2_reg=config.l2_reg,\n",
        "        )\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(config.learning_rate),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # Log model architecture\n",
        "        log_model_architecture(model, run, config)\n",
        "\n",
        "        # Prepare callbacks\n",
        "        callbacks = prepare_callbacks(run, fold)\n",
        "\n",
        "        # Prepare the data\n",
        "        decoder_input, decoder_target = prepare_decoder_data(y_train)\n",
        "        decoder_input_val, decoder_target_val = prepare_decoder_data(y_val)\n",
        "\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "        history = model.fit(\n",
        "            [X_train, decoder_input], decoder_target,\n",
        "            batch_size=config.batch_size,\n",
        "            epochs=config.epochs,\n",
        "            validation_data=([X_val, decoder_input_val], decoder_target_val),\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "        training_time = time.time() - start_time\n",
        "        run[\"metrics/training_time_seconds\"] = training_time\n",
        "\n",
        "        # Save model\n",
        "        model.save(f\"/content/drive/MyDrive/Dataset/MT-JavaIndo/model/{config.hidden_units}-{'Bahdanau' if config.use_attention else 'no-attention'}-{'Dropout' if config.use_dropout else 'no-dropout'}.keras\")\n",
        "\n",
        "        # Evaluate model\n",
        "        scores = model.evaluate([X_val, decoder_input_val], decoder_target_val, verbose=1)\n",
        "        fold_scores.append(scores)\n",
        "        fold_histories.append(history.history)\n",
        "\n",
        "        # Log final metrics\n",
        "        log_final_metrics(run, history)\n",
        "\n",
        "        # Create and log learning curves\n",
        "        plot_learning_curves(history, run)\n",
        "\n",
        "        run.stop()\n",
        "\n",
        "    # Summarize and return results\n",
        "    # summarize_training(neptune_runs, fold_histories, fold_scores)\n",
        "    return fold_histories, fold_scores, neptune_runs\n"
      ],
      "metadata": {
        "id": "SW86GhTy75Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "eK4mFUWT-GtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Path"
      ],
      "metadata": {
        "id": "T5g3RMFK--xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_folds = \"/content/drive/MyDrive/Dataset/MT-JavaIndo/nusa/dataset_kfolds (3).pkl\"\n",
        "PATH_tokenizer_info = \"/content/drive/MyDrive/Dataset/MT-JavaIndo/nusa/tokenizer_info (3).pkl\"\n"
      ],
      "metadata": {
        "id": "-OZGByd2_AKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Config"
      ],
      "metadata": {
        "id": "T1Dnn9tvBeU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define possible configurations\n",
        "hidden_units_list = [64]\n",
        "use_attention_list = [True]\n",
        "dropout_rate_list = [0.2]\n",
        "\n",
        "# Loop to create and train models for all configurations\n",
        "for hidden_units in hidden_units_list:\n",
        "    for use_attention in use_attention_list:\n",
        "        for dropout_rate in dropout_rate_list:\n",
        "            # Define version name\n",
        "            version_name = f\"{hidden_units}-{'Bahdanau' if use_attention else 'no-attention'}\"\n",
        "            if dropout_rate is not None:\n",
        "                version_name += f\"-dropout-{dropout_rate}\"\n",
        "\n",
        "            # Create config\n",
        "            config = ModelConfig(\n",
        "                hidden_units=hidden_units,\n",
        "                use_bidirectional=True,\n",
        "                use_attention=use_attention,\n",
        "                dropout_rate=dropout_rate if dropout_rate is not None else 0,\n",
        "                recurrent_dropout_rate=0.15,\n",
        "                l1_reg=1e-4,\n",
        "                l2_reg=1e-3,\n",
        "                use_dropout=dropout_rate is not None,\n",
        "                use_recurrent_dropout=False,\n",
        "                use_l1_regularization=False,\n",
        "                use_l2_regularization=False,\n",
        "                batch_size=64,\n",
        "                epochs=30,\n",
        "                learning_rate=1e-2,\n",
        "                selected_folds=4,\n",
        "                version=f\"MT-JavaIndo-v1.1-{version_name}\",\n",
        "                experiment_id=f\"{version_name}-experiment\",\n",
        "                config_filepath=\"/content/experiments\",\n",
        "\n",
        "                # NEPTUNE\n",
        "                neptune_project=\"ihsani.yulfa/Translation-Project\",\n",
        "                neptune_api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmMTAzYmRjZC01YjBlLTRhNDktOTZjYy00MDY4ODdkMzNjZTAifQ==\",\n",
        "            )\n",
        "\n",
        "            # Train the model with the current configuration\n",
        "            print(f\"Training model with configuration: {config.version}\")\n",
        "            histories, scores, runs = create_and_train_model_with_kfold(\n",
        "                config,\n",
        "                PATH_folds,\n",
        "                PATH_tokenizer_info\n",
        "            )\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'folder_name' with your folder path\n",
        "shutil.make_archive('/content/model', 'zip', 'model')\n",
        "files.download('model.zip')\n",
        "\n"
      ],
      "metadata": {
        "id": "_GdngKl_-HiV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95808bbd-dcb8-4df1-fe49-60891b609f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with configuration: MT-JavaIndo-v1.1-64-Bahdanau-dropout-0.2\n",
            "Configuration saved to experiments/config_MT-JavaIndo-v1.1-64-Bahdanau-dropout-0.2_64-Bahda.json\n",
            "downloading https://nlp.h-its.org/bpemb/jv/jv.wiki.bpe.vs200000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3734000/3734000 [00:01<00:00, 3299766.75B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs200000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3739973/3739973 [00:01<00:00, 3335184.57B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold 4:\n",
            "  X_train shape: (9576, 35)\n",
            "  X_val shape: (2393, 35)\n",
            "  y_train shape: (9576, 35)\n",
            "  y_val shape: (2393, 35)\n",
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/ihsani.yulfa/Translation-Project/e/TRAN-304\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │      \u001b[38;5;34m1,003,520\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_1           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │         \u001b[38;5;34m66,048\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │                        │\n",
              "│                           │ \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │                │                        │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]            │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m763,904\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m3\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m], │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m4\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),    │         \u001b[38;5;34m98,816\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │ \u001b[38;5;34m128\u001b[0m)]                  │                │ concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_layer_1         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),    │         \u001b[38;5;34m32,896\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)]      │                │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ attention_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11936\u001b[0m)    │      \u001b[38;5;34m3,067,552\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,003,520</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_1           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │                │                        │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]            │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">763,904</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                  │                │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_layer_1         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)]      │                │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ attention_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11936</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,067,552</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,032,736\u001b[0m (19.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,032,736</span> (19.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,032,736\u001b[0m (19.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,032,736</span> (19.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.5567 - loss: 3.7251 - val_accuracy: 0.6421 - val_loss: 2.5832 - learning_rate: 0.0100\n",
            "Epoch 2/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.6543 - loss: 2.3891 - val_accuracy: 0.6807 - val_loss: 2.2698 - learning_rate: 0.0100\n",
            "Epoch 3/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.7042 - loss: 1.9107 - val_accuracy: 0.7261 - val_loss: 1.9357 - learning_rate: 0.0100\n",
            "Epoch 4/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.7607 - loss: 1.4019 - val_accuracy: 0.7580 - val_loss: 1.7138 - learning_rate: 0.0100\n",
            "Epoch 5/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.8109 - loss: 1.0005 - val_accuracy: 0.7707 - val_loss: 1.6392 - learning_rate: 0.0100\n",
            "Epoch 6/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.8491 - loss: 0.7489 - val_accuracy: 0.7821 - val_loss: 1.5954 - learning_rate: 0.0100\n",
            "Epoch 7/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.8798 - loss: 0.5705 - val_accuracy: 0.7881 - val_loss: 1.5774 - learning_rate: 0.0100\n",
            "Epoch 8/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.8976 - loss: 0.4665 - val_accuracy: 0.7880 - val_loss: 1.5919 - learning_rate: 0.0100\n",
            "Epoch 9/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.9106 - loss: 0.3941 - val_accuracy: 0.7897 - val_loss: 1.6022 - learning_rate: 0.0100\n",
            "Epoch 10/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9185 - loss: 0.3515 - val_accuracy: 0.7912 - val_loss: 1.6117 - learning_rate: 0.0100\n",
            "Epoch 11/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9225 - loss: 0.3264 - val_accuracy: 0.7916 - val_loss: 1.6353 - learning_rate: 0.0100\n",
            "Epoch 12/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9303 - loss: 0.2859 - val_accuracy: 0.7898 - val_loss: 1.6686 - learning_rate: 0.0100\n",
            "Epoch 13/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.9407 - loss: 0.2399 - val_accuracy: 0.7969 - val_loss: 1.6223 - learning_rate: 0.0050\n",
            "Epoch 14/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9529 - loss: 0.1889 - val_accuracy: 0.7959 - val_loss: 1.6486 - learning_rate: 0.0050\n",
            "Epoch 15/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9565 - loss: 0.1726 - val_accuracy: 0.7962 - val_loss: 1.6607 - learning_rate: 0.0050\n",
            "Epoch 16/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9593 - loss: 0.1606 - val_accuracy: 0.7964 - val_loss: 1.6742 - learning_rate: 0.0050\n",
            "Epoch 17/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9600 - loss: 0.1535 - val_accuracy: 0.7960 - val_loss: 1.6895 - learning_rate: 0.0050\n",
            "Epoch 18/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.9649 - loss: 0.1375 - val_accuracy: 0.7984 - val_loss: 1.6977 - learning_rate: 0.0025\n",
            "Epoch 19/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9678 - loss: 0.1264 - val_accuracy: 0.7979 - val_loss: 1.6961 - learning_rate: 0.0025\n",
            "Epoch 20/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9697 - loss: 0.1177 - val_accuracy: 0.7977 - val_loss: 1.7159 - learning_rate: 0.0025\n",
            "Epoch 21/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9708 - loss: 0.1126 - val_accuracy: 0.7965 - val_loss: 1.7323 - learning_rate: 0.0025\n",
            "Epoch 22/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9718 - loss: 0.1086 - val_accuracy: 0.7981 - val_loss: 1.7378 - learning_rate: 0.0025\n",
            "Epoch 23/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9740 - loss: 0.1011 - val_accuracy: 0.7979 - val_loss: 1.7367 - learning_rate: 0.0012\n",
            "Epoch 24/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.9759 - loss: 0.0945 - val_accuracy: 0.7985 - val_loss: 1.7439 - learning_rate: 0.0012\n",
            "Epoch 25/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9754 - loss: 0.0947 - val_accuracy: 0.7990 - val_loss: 1.7384 - learning_rate: 0.0012\n",
            "Epoch 26/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9755 - loss: 0.0948 - val_accuracy: 0.7978 - val_loss: 1.7505 - learning_rate: 0.0012\n",
            "Epoch 27/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9769 - loss: 0.0899 - val_accuracy: 0.7984 - val_loss: 1.7501 - learning_rate: 0.0012\n",
            "Epoch 28/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.9779 - loss: 0.0875 - val_accuracy: 0.7989 - val_loss: 1.7537 - learning_rate: 6.2500e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9785 - loss: 0.0838 - val_accuracy: 0.7992 - val_loss: 1.7545 - learning_rate: 6.2500e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9788 - loss: 0.0832 - val_accuracy: 0.7988 - val_loss: 1.7579 - learning_rate: 6.2500e-04\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8018 - loss: 1.7078\n",
            "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
            "[neptune] [info   ] Done!\n",
            "[neptune] [info   ] Waiting for the remaining 6 operations to synchronize with Neptune. Do not kill this process.\n",
            "[neptune] [info   ] All 6 operations synced, thanks for waiting!\n",
            "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/ihsani.yulfa/Translation-Project/e/TRAN-304/metadata\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_54a7761c-632d-4962-a27d-1fad177122f1\", \"model.zip\", 22)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}